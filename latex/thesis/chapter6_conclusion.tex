\chapter{Conclusion}\label{ch:conclusion}
\setlength{\epigraphwidth}{0.90\textwidth}
\epigraph{``We are all shaped by the tools we use, in particular: the formalisms we use shape our thinking habits, for better or for worse, and that means that we have to be very careful in the choice of what we learn and teach, for unlearning is not really possible.''}{\begin{flushright}--Edsger W. \citet{dijkstra2000answers}, \href{https://www.cs.utexas.edu/~EWD/transcriptions/EWD13xx/EWD1305.html}{\textit{Answers to questions from students of Software Engineering}}\end{flushright}}

In this work, we explored four different programming tools from software engineering for the development of intelligent systems, broadly addressing cognitive complexity arising in three phases of Royce's Waterfall method~\autoref{fig:waterfall_model}. These tools have varying degrees of practicality, from highly experimental (adversarial testing of differentiable programs ~\autoref{ch:difftest}) to more pragmatic (containerization ~\autoref{ch:ducker}). In each, we provide some motivating examples and use cases which demonstrate key deficiencies in the current state of programming tools for intelligent systems and propose candidate solutions which address a few of those shortcomings. While we certainly hope that intelligent system programmers (e.g. roboticists and machine learning practitioners) may derive some value from the tools themselves, our intention is to be \textit{instructive} rather than \textit{prescriptive}.

In building tools and validating their effectiveness for toy applications, we hope that middleware and tools developers will carefully consider the cognitive complexity which software abstraction can introduce and the importance of notational and denotational design practices. In addition, we hope that by providing some examples illustrating programming tools machine learning, developers will be inspired to re-imagine the possibilities for computer-aided programming towards the design of intelligent systems and begin to design better tools for developers working on similar problems.

By complementing the cognitive abilities of human programmers -- who excel at creative problem solving and high-level abstract reasoning -- with the low-level symbolic processing capabilities of programming tools, we can accelerate the design, development and validation of intelligent systems in real-world applications. This process, we argue, deserves more specific tools than general-purpose programming due to the opportunities and challenges which intelligent systems present and the unique interplay of human-machine intelligence.

As we start to engineer autonomous systems which take increasingly human decisions, programmers will play a critical role in shaping the behavior and dynamics of these systems. In order to build trustworthy autonomous systems, it will be important to have tools which enable humans to understand and inform their behavior in a more rigorous manner than monitoring training curves and performing hyperparameter sweeps. This requires us to actively rethink the programming model in machine learning to incorporate human knowledge, e.g. using differentiable programming and type theory (\autoref{ch:kotlingrad}) or building custom tools which incorporate automatic reasoning capabilities and visualization tools (e.g. customized run and debugging assistance \autoref{ch:hatchery}). Finally, to make the resulting software artifacts more reproducible will require sound build systems and best practices for reproducible software installation and configuration (\autoref{ch:ducker}).

Traditional software engineering prescribes a rigorous process model and testing methodology~\autoref{fig:waterfall_model} which has guided generations of traditional software products. In order to become a true engineering discipline, intelligent systems will need to re-imagine these ideas for systems which continously adapt to their environment. Intelligent systems are trained on objective functions, which are typically one- or low-dimensional metrics for evaluating the performance of the system, typically in the form of a scalar value known as \textit{error} or \textit{loss}. In practice, we care about a diverse set of phenomena including latency, cost, risk, usability, understandability, energy efficiency, trustworthiness, and combinations of many other criteria.

In traditional software engineering, it is reasonable to assume the people who are implementing a new system have some implicit domain knowledge and are well-intentioned human beings working towards a common goal -- given a coarse description, they can fill in the blanks. When building an intelligent system, it is more accurate to assume the entity implementing our requirements is a na\"ive but crafty genie. Given some data and an optimization metric, it will take every available shortcut to satisfy the desired criteria. If we are not careful about constructing the requirements correctly, this entity can create a solution that simply does not work (in the best case), or appears to work but is actually cursed in a subtle manner.

When building an intelligent system developers must carefully ask, ``What are the behavioral requirements of the system?'' This question is often very troublesome, for the requirements cannot be fuzzy specifications, but precise constraints on the solution set. Specifying the requirements is often indistinguishable from implementing the system -- with the right language abstractions (e.g.\ declarative programming), requirements and implementation can even take the same notation (e.g. SQL, Prolog). But how can we be assured the system meets those requirements? Humans can drive a car, but have difficulty describing the algorithm for driving. Labeling the data by hand is too expensive. Exhaustive verification is right out the window. However fuzz testing remains an attractive alternative. As we show in \autoref{sec:prob_ad_test}, by making practical assumptions about the data and oracle, we can spend the available computational budget more wisely to detect more severe errors with lower fiscal and computational overhead.

For example, in the design of a web-based advertisement recommendation system, we can optimize for various criteria such as click rate, engagement, and sales conversion. So long as we can measure these parameters, modern function approximators can optimize for any single criterion or combination thereof (\autoref{eq:moo_spec}). Much of the work involved in machine learning is designing representations which are suitable for downstream tasks and designing loss functions which accurately capture those tasks. For example, by optimizing for click rate, we create an artificial market for click bots. Similarly, in self-driving vehicles, we often want to optimize for passenger safety. However, by doing so na\"ively can train a vehicle that never moves, or always yields to passing vehicles. Building representations and loss functions which capture the full range of objectives can be a painstaking process.